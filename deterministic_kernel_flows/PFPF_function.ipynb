{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PFPF Filter: Function-Based Implementation\n",
    "\n",
    "This notebook implements the **Particle Flow Particle Filter (PFPF)** using **pure functions** (no classes).\n",
    "\n",
    "Each function directly corresponds to specific lines in **Algorithm 1 (PF-PF LEDH)** and **Algorithm 2 (PF-PF EDH)** from Li & Coates (2017).\n",
    "\n",
    "## Algorithm 1: PF-PF (LEDH) - Local Linearization\n",
    "\n",
    "```\n",
    "Initialization:\n",
    "  1  Draw {x_0^i}_{i=1}^Np from the prior p_0(x)\n",
    "  2  Set {w_0^i}_{i=1}^Np = 1/Np\n",
    "  3  for k = 1 to K do\n",
    "  4    for i = 1, ..., Np do\n",
    "  5      Apply EKF/UKF prediction: (x_{k-1}^i, P_{k-1}^i) → (m_{k|k-1}^i, P^i)\n",
    "  6      Calculate η̄^i = g_k(x_{k-1}^i, 0)\n",
    "  7      Propagate particles: η_0^i = g_k(x_{k-1}^i, v_k)\n",
    "  8      Set η_1^i = η_0^i and θ^i = 1\n",
    "  9      Calculate η̄_0^i = g_k(x_{k-1}^i, 0)\n",
    " 10    endfor\n",
    " 11    Set λ = 0\n",
    " 12    for j = 1, ..., N_λ do\n",
    " 13      Set λ = λ + ε_j\n",
    " 14      for i = 1, ..., Np do\n",
    " 15        Set η̄_0 = η̄_0^i and P = P^i\n",
    " 16        Calculate A_j^i(λ) and b_j^i(λ) with linearization at η̄^i\n",
    " 17        Migrate η̄^i: η̄^i = η̄^i + ε_j(A_j^i(λ)η̄^i + b_j^i(λ))\n",
    " 18        Migrate particles: η_1^i = η_1^i + ε_j(A_j^i(λ)η_1^i + b_j^i(λ))\n",
    " 19        Calculate θ^i = θ^i / |det(I + ε_j A_j^i(λ))|\n",
    " 20      endfor\n",
    " 21    endfor\n",
    " 22    for i = 1, ..., Np do\n",
    " 23      Set x_k^i = η_1^i\n",
    " 24      w_k^i = [p(x_k^i|x_{k-1}^i)p(z_k|x_k^i)θ^i / p(η_0^i|x_{k-1}^i)] w_{k-1}^i\n",
    " 25    endfor\n",
    " 26    Normalize weights\n",
    " 27    Apply EKF/UKF update for each particle\n",
    " 28    Estimate x̂_k\n",
    " 29    (Optional) Resample\n",
    " 30  endfor\n",
    "```\n",
    "\n",
    "## Algorithm 2: PF-PF (EDH) - Global Linearization\n",
    "\n",
    "```\n",
    "Initialization:\n",
    "  1  Draw {x_0^i}_{i=1}^Np from p_0(x)\n",
    "  2  Set {w_0^i}_{i=1}^Np = 1/Np\n",
    "  3  for k = 1 to K do\n",
    "  4    Apply EKF/UKF prediction: (x̂_{k-1}, P_{k-1}) → (m_{k|k-1}, P)\n",
    "  5    for i = 1, ..., Np do\n",
    "  6      Propagate particles: η_0^i = g_k(x_{k-1}^i, v_k)\n",
    "  7      Set η_1^i = η_0^i\n",
    "  8    endfor\n",
    "  9    Calculate η̄_0 = g_k(x̂_{k-1}, 0) and set η̄ = η̄_0\n",
    " 10    Set λ = 0\n",
    " 11    for j = 1, ..., N_λ do\n",
    " 12      Set λ = λ + ε_j\n",
    " 13      Calculate A_j(λ) and b_j(λ) with linearization at η̄\n",
    " 14      Migrate η̄: η̄ = η̄ + ε_j(A_j(λ)η̄ + b_j(λ))\n",
    " 15      for i = 1, ..., Np do\n",
    " 16        Migrate particles: η_1^i = η_1^i + ε_j(A_j(λ)η_1^i + b_j(λ))\n",
    " 17      endfor\n",
    " 18    endfor\n",
    " 19    for i = 1, ..., Np do\n",
    " 20      Set x_k^i = η_1^i\n",
    " 21      w_k^i = [p(x_k^i|x_{k-1}^i)p(z_k|x_k^i) / p(η_0^i|x_{k-1}^i)] w_{k-1}^i\n",
    " 22    endfor\n",
    " 23    Normalize weights\n",
    " 24    Apply EKF/UKF update: (m_{k|k-1}, P) → (m_{k|k}, P_k)\n",
    " 25    Estimate x̂_k\n",
    " 26    (Optional) Resample\n",
    " 27  endfor\n",
    "```\n",
    "\n",
    "## References\n",
    "\n",
    "- **Li & Coates (2017)**: \"Particle Filtering with Invertible Particle Flow\", arXiv:1607.08799\n",
    "- **MATLAB Code**: Algorithm 1 and Algorithm 2 in the paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import acoustic model functions\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from acoustic_function import (\n",
    "    initialize_acoustic_model,\n",
    "    state_transition,\n",
    "    observation_model,\n",
    "    compute_observation_jacobian,\n",
    "    simulate_trajectory\n",
    ")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {len(tf.config.list_physical_devices('GPU')) > 0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize Acoustic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize acoustic model with default parameters (4 targets, 25 sensors)\n",
    "model_params = initialize_acoustic_model(n_targets=4)\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Targets: {model_params['n_targets']}\")\n",
    "print(f\"  Sensors: {model_params['n_sensors']} (5×5 grid)\")\n",
    "print(f\"  State dimension: {model_params['state_dim']}\")\n",
    "print(f\"  Surveillance area: {model_params['sim_area_size']}m × {model_params['sim_area_size']}m\")\n",
    "print(f\"\\nAcoustic Parameters:\")\n",
    "print(f\"  Amplitude (Ψ): {model_params['amplitude'].numpy()}\")\n",
    "print(f\"  Measurement noise std: {model_params['measurement_noise_std']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PFPF Filter Functions\n",
    "\n",
    "Each function below corresponds to specific lines in Algorithm 1 (LEDH) and Algorithm 2 (EDH)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function 1: Compute Lambda Steps\n",
    "\n",
    "**Purpose**: Compute exponentially spaced lambda steps for particle flow.\n",
    "\n",
    "**Algorithm**: Pre-processing (before Algorithm 1/2)\n",
    "\n",
    "**Equations**:\n",
    "- Step sizes: $\\epsilon_j = \\epsilon_1 \\cdot q^{j-1}$ for $j = 1, ..., N_\\lambda$\n",
    "- Constraint: $\\sum_{j=1}^{N_\\lambda} \\epsilon_j = 1$\n",
    "- Solution: $\\epsilon_1 = \\frac{1-q}{1-q^{N_\\lambda}}$ where $q > 1$ (typically $q = 1.2$)\n",
    "- Lambda values: $\\lambda_j = \\sum_{i=1}^{j} \\epsilon_i$\n",
    "\n",
    "**Reference**: Same as EDH filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_lambda_steps(n_lambda, lambda_ratio):\n",
    "    \"\"\"\n",
    "    Compute exponentially spaced lambda steps.\n",
    "    \n",
    "    Args:\n",
    "        n_lambda: Number of lambda steps (typically 20)\n",
    "        lambda_ratio: Ratio for exponential spacing (typically 1.2)\n",
    "    \n",
    "    Returns:\n",
    "        lambda_steps: Step sizes ε_j, shape (n_lambda,)\n",
    "        lambda_values: Cumulative lambda values λ_j, shape (n_lambda,)\n",
    "    \"\"\"\n",
    "    q = lambda_ratio\n",
    "    n = n_lambda\n",
    "    \n",
    "    # Initial step size: ε_1 = (1-q)/(1-q^n)\n",
    "    epsilon_1 = (1 - q) / (1 - q**n)\n",
    "    \n",
    "    # Step sizes: ε_j = ε_1 * q^(j-1) for j=1,...,n\n",
    "    step_sizes = [epsilon_1 * (q**j) for j in range(n)]\n",
    "    lambda_steps = tf.constant(step_sizes, dtype=tf.float32)\n",
    "    \n",
    "    # Cumulative lambda values: λ_j = Σ_{i=1}^j ε_i\n",
    "    lambda_values = tf.cumsum(lambda_steps)\n",
    "    \n",
    "    return lambda_steps, lambda_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function 2: Initialize Particles\n",
    "\n",
    "**Algorithm Lines 1-2 (both LEDH and EDH)**\n",
    "\n",
    "**Purpose**: Draw initial particles from prior distribution and initialize weights.\n",
    "\n",
    "**Equations**:\n",
    "- Draw: $x_0^{(i)} \\sim \\mathcal{N}(x_0, P_0)$ for $i = 1, ..., N$\n",
    "- Initial weights: $w_0^{(i)} = 1/N_p$\n",
    "- Initial mean: $x_0$\n",
    "- Initial covariance: $P_0 = \\text{diag}(\\sigma_0^2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_particles(model_params, n_particle):\n",
    "    \"\"\"\n",
    "    Initialize particles from Gaussian prior.\n",
    "\n",
    "    Algorithm Lines 1-2:\n",
    "      1  Draw {x_0^i}_{i=1}^N from the prior p_0(x)\n",
    "      2  Set {w_0^i}_{i=1}^N = 1/Np\n",
    "\n",
    "    Args:\n",
    "        model_params: Dictionary from initialize_acoustic_model()\n",
    "        n_particle: Number of particles\n",
    "\n",
    "    Returns:\n",
    "        particles: Initial particles, shape (state_dim, n_particle)\n",
    "        weights: Initial weights, shape (n_particle,)\n",
    "        m0: Random initial mean, shape (state_dim, 1)\n",
    "        P0: Initial covariance, shape (state_dim, state_dim)\n",
    "    \"\"\"\n",
    "    state_dim = model_params['state_dim']\n",
    "    x0 = model_params['x0_initial_target_states']  # (state_dim, 1)\n",
    "    n_targets = model_params['n_targets']\n",
    "    sim_area_size = model_params['sim_area_size']\n",
    "\n",
    "    # Initial uncertainty from paper\n",
    "    sigma0_single = tf.constant(\n",
    "        [0.1**0.5, 0.1**0.5, 0.0005**0.5, 0.0005**0.5],\n",
    "        dtype=tf.float32\n",
    "    )    \n",
    "    sigma0 = tf.tile(sigma0_single, [n_targets])\n",
    "    # Line 2: P_0 = diag(σ_0^2)\n",
    "    P0 = tf.linalg.diag(tf.square(sigma0))\n",
    "\n",
    "    # Sample random mean m0 and ensure it's within bounds\n",
    "    out_of_bound = True\n",
    "    while out_of_bound:\n",
    "        noise = tf.random.normal((state_dim, 1), dtype=tf.float32)\n",
    "        m0 = x0 + tf.expand_dims(sigma0, 1) * noise\n",
    "\n",
    "        # Check if all target positions are within surveillance region\n",
    "        x_positions = m0[0::4, 0]  # indices 0, 4, 8, 12, ... (x positions)\n",
    "        y_positions = m0[1::4, 0]  # indices 1, 5, 9, 13, ... (y positions)\n",
    "\n",
    "        # Check bounds: all positions should be in [0, sim_area_size]\n",
    "        x_in_bounds = tf.reduce_all(x_positions >= 0.0) and tf.reduce_all(x_positions <= sim_area_size)\n",
    "        y_in_bounds = tf.reduce_all(y_positions >= 0.0) and tf.reduce_all(y_positions <= sim_area_size)\n",
    "\n",
    "        if x_in_bounds and y_in_bounds:\n",
    "            out_of_bound = False\n",
    "\n",
    "    # Sample particles around m0\n",
    "    noise = tf.random.normal((state_dim, n_particle), dtype=tf.float32)\n",
    "    particles = m0 + tf.expand_dims(sigma0, 1) * noise\n",
    "    \n",
    "    # Initialize uniform weights\n",
    "    weights = tf.ones(n_particle, dtype=tf.float32) / n_particle\n",
    "\n",
    "    return particles, weights, m0, P0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function 3: Propagate Particles\n",
    "\n",
    "**Algorithm Line 7 (LEDH) / Line 6 (EDH)**\n",
    "\n",
    "**Purpose**: Propagate particles through motion model.\n",
    "\n",
    "**Equation**:\n",
    "$$\\eta_0^{(i)} = g_k(x_{k-1}^{(i)}, v_k), \\quad v_k \\sim \\mathcal{N}(0, Q)$$\n",
    "\n",
    "where:\n",
    "- $g_k$ is the state transition function\n",
    "- $Q$ is the process noise covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagate_particles(particles, model_params):\n",
    "    \"\"\"\n",
    "    Propagate particles through motion model.\n",
    "    \n",
    "    Algorithm Line 7 (LEDH) / Line 6 (EDH):\n",
    "      Propagate particles η_0^i = g_k(x_{k-1}^i, v_k)\n",
    "    \n",
    "    Equation:\n",
    "      x_k = Φ * x_{k-1} + w_k,  where w_k ~ N(0, Q)\n",
    "    \n",
    "    Args:\n",
    "        particles: Current particles, shape (state_dim, n_particle)\n",
    "        model_params: Dictionary with 'Phi' and 'Q'\n",
    "    \n",
    "    Returns:\n",
    "        particles_pred: Predicted particles, shape (state_dim, n_particle)\n",
    "    \"\"\"\n",
    "    state_dim = model_params['state_dim']\n",
    "    Phi = model_params['Phi']  # State transition matrix\n",
    "    Q = model_params['Q']      # Process noise covariance\n",
    "    n_particle = tf.shape(particles)[1]\n",
    "    \n",
    "    # Linear propagation: x_k = Φ * x_{k-1}\n",
    "    particles_pred = tf.matmul(Phi, particles)\n",
    "    \n",
    "    # Add process noise: w_k ~ N(0, Q)\n",
    "    Q_chol = tf.linalg.cholesky(Q)\n",
    "    noise = tf.matmul(Q_chol, tf.random.normal((state_dim, n_particle), dtype=tf.float32))\n",
    "    \n",
    "    return particles_pred + noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function 4: Estimate Covariance from Particles\n",
    "\n",
    "**Algorithm Lines 5 (LEDH) / Line 4 (EDH)**\n",
    "\n",
    "**Purpose**: Estimate covariance matrix from particle spread.\n",
    "\n",
    "**Equation**:\n",
    "$$P = \\frac{1}{N-1} \\sum_{i=1}^N (x^{(i)} - \\bar{x})(x^{(i)} - \\bar{x})^T$$\n",
    "\n",
    "where $\\bar{x} = \\frac{1}{N} \\sum_{i=1}^N x^{(i)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_covariance(particles):\n",
    "    \"\"\"\n",
    "    Estimate covariance from particles.\n",
    "    \n",
    "    Algorithm Line 5 (LEDH) / Line 4 (EDH): P prediction\n",
    "    \n",
    "    Equation:\n",
    "      P = (1/(N-1)) * Σ (x_i - x̄)(x_i - x̄)^T\n",
    "    \n",
    "    Args:\n",
    "        particles: Particles, shape (state_dim, n_particle)\n",
    "    \n",
    "    Returns:\n",
    "        P: Covariance matrix, shape (state_dim, state_dim)\n",
    "    \"\"\"\n",
    "    # Compute mean\n",
    "    mean = tf.reduce_mean(particles, axis=1, keepdims=True)\n",
    "    \n",
    "    # Center particles\n",
    "    centered = particles - mean\n",
    "    \n",
    "    # Covariance: P = (1/(N-1)) * centered @ centered^T\n",
    "    n_particles = tf.cast(tf.shape(particles)[1], tf.float32)\n",
    "    P = tf.matmul(centered, tf.transpose(centered)) / (n_particles - 1.0)\n",
    "    \n",
    "    # Add small regularization for numerical stability\n",
    "    P = P + 1e-6 * tf.eye(tf.shape(P)[0], dtype=tf.float32)\n",
    "    \n",
    "    return P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function 5: Compute Flow Parameters A and b\n",
    "\n",
    "**Algorithm Line 16 (LEDH) / Line 13 (EDH)**\n",
    "\n",
    "**Purpose**: Compute flow matrix $A(\\lambda)$ and flow vector $b(\\lambda)$ for particle flow.\n",
    "\n",
    "**Equations** (from Li & Coates 2017, following EDH equations):\n",
    "\n",
    "$$A(\\lambda) = -\\frac{1}{2} P H^T (\\lambda H P H^T + R)^{-1} H$$\n",
    "\n",
    "$$b(\\lambda) = (I + 2\\lambda A)[(I + \\lambda A) P H^T R^{-1} (z - e) + A\\bar{\\eta}_0]$$\n",
    "\n",
    "where:\n",
    "- $H = \\frac{\\partial h}{\\partial x}|_{\\bar{\\eta}}$ is the observation Jacobian\n",
    "- $e = h(\\bar{\\eta}) - H\\bar{\\eta}$ is the linearization residual\n",
    "- $z$ is the current measurement\n",
    "- $P$ is the covariance (should be $P_{k|k-1}$)\n",
    "- $R$ is the measurement noise covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_flow_parameters(linearization_point, x_bar_mu_0, P, measurement, lam, model_params):\n",
    "    \"\"\"\n",
    "    Compute flow parameters A and b for particle flow.\n",
    "    \n",
    "    Algorithm Line 16 (LEDH) / Line 13 (EDH):\n",
    "      Calculate A and b from EDH equations\n",
    "    \n",
    "    Equations:\n",
    "      A(λ) = -1/2 * P * H^T * (λ*H*P*H^T + R)^{-1} * H\n",
    "      b(λ) = (I + 2λA) * [(I + λA) * P*H^T*R^{-1}*(z-e) + A*x̄]\n",
    "    \n",
    "    Args:\n",
    "        linearization_point: Particle position (state_dim, 1) - used for linearization\n",
    "        x_bar_mu_0: Mean trajectory (state_dim, 1) - used in b computation\n",
    "        P: Covariance (state_dim, state_dim) - should be P_{k|k-1}\n",
    "        measurement: Current measurement z, shape (n_sensor, 1)\n",
    "        lam: Current lambda value (scalar)\n",
    "        model_params: Dictionary with observation model\n",
    "    \n",
    "    Returns:\n",
    "        A: Flow matrix, shape (state_dim, state_dim)\n",
    "        b: Flow vector, shape (state_dim,)\n",
    "    \"\"\"\n",
    "    state_dim = tf.shape(P)[0]\n",
    "    R = model_params['R']\n",
    "    \n",
    "    # Calculate H_x by linearizing at linearization_point\n",
    "    H = compute_observation_jacobian(linearization_point, model_params)\n",
    "    \n",
    "    # Compute h(x̄) and linearization residual: e = h(x̄) - H*x̄\n",
    "    h_x_bar = observation_model(linearization_point, model_params, no_noise=True)\n",
    "    h_x_bar = tf.squeeze(h_x_bar, axis=1)  # (n_sensor,)\n",
    "    e = h_x_bar - tf.linalg.matvec(H, tf.squeeze(linearization_point, axis=1))\n",
    "    \n",
    "    # Compute H*P*H^T\n",
    "    HPHt = tf.matmul(tf.matmul(H, P), tf.transpose(H))\n",
    "    \n",
    "    # Innovation covariance: S = λ*H*P*H^T + R\n",
    "    n_sensor = tf.shape(R)[0]\n",
    "    regularization = 1e-6 * tf.eye(n_sensor, dtype=tf.float32)\n",
    "    S = lam * HPHt + R + regularization\n",
    "    \n",
    "    # Use Cholesky decomposition instead of direct inversion (more stable)\n",
    "    S_chol = tf.linalg.cholesky(S)\n",
    "    \n",
    "    # Compute P*H^T\n",
    "    PHt = tf.matmul(P, tf.transpose(H))\n",
    "    \n",
    "    # Equation: Flow matrix A = -0.5 * P*H^T * S^{-1} * H\n",
    "    S_inv_H = tf.linalg.cholesky_solve(S_chol, H)\n",
    "    A = -0.5 * tf.matmul(PHt, S_inv_H)\n",
    "    \n",
    "    # Innovation: z - e\n",
    "    innovation = tf.squeeze(measurement, axis=1) - e\n",
    "    \n",
    "    # Compute R^{-1}*(z - e) using Cholesky decomposition\n",
    "    R_regularized = R + regularization\n",
    "    R_chol = tf.linalg.cholesky(R_regularized)\n",
    "    R_inv_innov = tf.linalg.cholesky_solve(R_chol, tf.expand_dims(innovation, 1))\n",
    "    R_inv_innov = tf.squeeze(R_inv_innov, axis=1)\n",
    "    \n",
    "    # Identity matrix\n",
    "    I = tf.eye(state_dim, dtype=tf.float32)\n",
    "    \n",
    "    # (I + λA) and (I + 2λA)\n",
    "    I_plus_lam_A = I + lam * A\n",
    "    I_plus_2lam_A = I + 2 * lam * A\n",
    "    \n",
    "    # Equation: Flow vector b\n",
    "    # First term: (I + λA) * P*H^T * R^{-1}*(z - e)\n",
    "    term1 = tf.linalg.matvec(tf.matmul(I_plus_lam_A, PHt), R_inv_innov)\n",
    "    # Second term: A * x̄\n",
    "    term2 = tf.linalg.matvec(A, tf.squeeze(x_bar_mu_0, axis=1))\n",
    "    # Combined: b = (I + 2λA) * [term1 + term2]\n",
    "    b = tf.linalg.matvec(I_plus_2lam_A, term1 + term2)\n",
    "    \n",
    "    return A, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function 6: Particle Flow (LEDH - Local Linearization)\n",
    "\n",
    "**Algorithm Lines 11-21 (LEDH)**\n",
    "\n",
    "**Purpose**: Migrate particles from prior to posterior using particle flow with **local linearization**.\n",
    "\n",
    "**Flow ODE**:\n",
    "$$\\frac{d\\eta}{d\\lambda} = A^i(\\lambda) \\eta^i + b^i(\\lambda)$$\n",
    "\n",
    "**Numerical integration** (Euler method):\n",
    "$$\\eta^{i,(j+1)} = \\eta^{i,(j)} + \\epsilon_j [A^i(\\lambda_j) \\eta^{i,(j)} + b^i(\\lambda_j)]$$\n",
    "\n",
    "where $A^i$ and $b^i$ are computed with linearization at each particle position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def particle_flow_ledh(particles, measurement, P_pred, lambda_steps, lambda_values,\n",
    "                       eta_bar_mu_0_list, model_params):\n",
    "    \"\"\"\n",
    "    Migrate particles from prior to posterior using LEDH flow (local linearization).\n",
    "\n",
    "    Algorithm Lines 11-21 (LEDH):\n",
    "      11  Set λ = 0\n",
    "      12  for j = 1, ..., N_λ do\n",
    "      13    Set λ = λ + ε_j\n",
    "      14    for i = 1, ..., Np do\n",
    "      15      Set η̄_0 = η̄_0^i and P = P^i\n",
    "      16      Calculate A_j^i(λ) and b_j^i(λ) with linearization at η̄^i\n",
    "      17      Migrate η̄^i\n",
    "      18      Migrate particles\n",
    "      19      Calculate θ^i = θ^i / |det(I + ε_j A_j^i(λ))|\n",
    "      20    endfor\n",
    "      21  endfor\n",
    "\n",
    "    Args:\n",
    "        particles: Predicted particles, shape (state_dim, n_particle)\n",
    "        measurement: Current measurement, shape (n_sensor, 1)\n",
    "        P_pred: List of prior covariances P^i, each shape (state_dim, state_dim)\n",
    "        lambda_steps: Step sizes ε_j, shape (n_lambda,)\n",
    "        lambda_values: Cumulative lambda values λ_j, shape (n_lambda,)\n",
    "        eta_bar_mu_0_list: List of mean trajectories η̄_0^i, each shape (state_dim, 1)\n",
    "        model_params: Dictionary with observation model\n",
    "\n",
    "    Returns:\n",
    "        particles_flowed: Updated particles, shape (state_dim, n_particle)\n",
    "        theta_values: Determinant products for weight update, shape (n_particle,)\n",
    "    \"\"\"\n",
    "    # Initialize flow\n",
    "    eta = tf.identity(particles)  # Current particle positions\n",
    "    n_particle = tf.shape(particles)[1].numpy()\n",
    "    n_lambda = len(lambda_steps)\n",
    "    state_dim = tf.shape(particles)[0].numpy()\n",
    "    \n",
    "    # Initialize auxiliary trajectories for linearization points\n",
    "    eta_bar_list = [tf.identity(eta_bar_mu_0_list[i]) for i in range(n_particle)]\n",
    "    \n",
    "    # Initialize theta values (determinant products)\n",
    "    theta_values = tf.ones(n_particle, dtype=tf.float32)\n",
    "    \n",
    "    # Algorithm Line 11: Set λ = 0\n",
    "    # Algorithm Line 12: for j = 1, ..., N_λ do\n",
    "    for j in range(n_lambda):\n",
    "        # Algorithm Line 13: Set λ = λ + ε_j\n",
    "        epsilon_j = lambda_steps[j].numpy()   # Step size\n",
    "        lambda_j = lambda_values[j]   # Current lambda value\n",
    "        \n",
    "        # Store updated values for this step\n",
    "        theta_updates = []\n",
    "        eta_updates = []\n",
    "        eta_bar_updates = []\n",
    "        \n",
    "        # Algorithm Line 14: for i = 1, ..., Np do\n",
    "        for i in range(n_particle):\n",
    "            # Algorithm Line 15: Set η̄_0 = η̄_0^i and P = P^i\n",
    "            x_i = tf.expand_dims(eta[:, i], 1)  # (state_dim, 1)\n",
    "            eta_bar_i = eta_bar_list[i]  # (state_dim, 1)\n",
    "            P_i = P_pred[i]  # (state_dim, state_dim)\n",
    "            eta_bar_mu_0_i = eta_bar_mu_0_list[i]  # (state_dim, 1)\n",
    "            \n",
    "            # Algorithm Line 16: Compute A_i, b_i for THIS particle\n",
    "            # Linearization at η̄^i (auxiliary trajectory)\n",
    "            A_i, b_i = compute_flow_parameters(eta_bar_i, eta_bar_mu_0_i, P_i, measurement,\n",
    "                                               lambda_j, model_params)\n",
    "            \n",
    "            # Algorithm Line 17: Migrate η̄^i\n",
    "            slope_bar = tf.linalg.matvec(A_i, tf.squeeze(eta_bar_i)) + b_i\n",
    "            eta_bar_new = eta_bar_i + epsilon_j * tf.expand_dims(slope_bar, 1)\n",
    "            \n",
    "            # Algorithm Line 18: Migrate particles\n",
    "            slope_i = tf.linalg.matvec(A_i, tf.squeeze(x_i)) + b_i\n",
    "            x_new = x_i + epsilon_j * tf.expand_dims(slope_i, 1)\n",
    "            \n",
    "            # Algorithm Line 19: Calculate θ^i = θ^i / |det(I + ε_j A_j^i(λ))|\n",
    "            I = tf.eye(state_dim, dtype=tf.float32)\n",
    "            det_jacobian = tf.abs(tf.linalg.det(I + epsilon_j * A_i))\n",
    "            theta_new = theta_values[i] / det_jacobian\n",
    "            \n",
    "            # Store updates\n",
    "            theta_updates.append(theta_new)\n",
    "            eta_updates.append(tf.squeeze(x_new, axis=1))\n",
    "            eta_bar_updates.append(eta_bar_new)\n",
    "        \n",
    "        # Update all particles and theta values\n",
    "        theta_values = tf.stack(theta_updates)\n",
    "        eta = tf.stack(eta_updates, axis=1)\n",
    "        eta_bar_list = eta_bar_updates\n",
    "    \n",
    "    return eta, theta_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function 7: Particle Flow (EDH - Global Linearization)\n",
    "\n",
    "**Algorithm Lines 10-18 (EDH)**\n",
    "\n",
    "**Purpose**: Migrate particles from prior to posterior using particle flow with **global linearization**.\n",
    "\n",
    "**Flow ODE**:\n",
    "$$\\frac{d\\eta}{d\\lambda} = A(\\lambda) \\eta + b(\\lambda)$$\n",
    "\n",
    "**Key difference from LEDH**: Uses the same $A$ and $b$ for all particles, computed at the global mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def particle_flow_edh(particles, measurement, P_pred, lambda_steps, lambda_values,\n",
    "                      eta_bar_mu_0, model_params):\n",
    "    \"\"\"\n",
    "    Migrate particles from prior to posterior using EDH flow (global linearization).\n",
    "\n",
    "    Algorithm Lines 10-18 (EDH):\n",
    "      10  Set λ = 0\n",
    "      11  for j = 1, ..., N_λ do\n",
    "      12    Set λ = λ + ε_j\n",
    "      13    Calculate A_j(λ) and b_j(λ) with linearization at η̄\n",
    "      14    Migrate η̄\n",
    "      15    for i = 1, ..., Np do\n",
    "      16      Migrate particles\n",
    "      17    endfor\n",
    "      18  endfor\n",
    "\n",
    "    Args:\n",
    "        particles: Predicted particles, shape (state_dim, n_particle)\n",
    "        measurement: Current measurement, shape (n_sensor, 1)\n",
    "        P_pred: Prior covariance P, shape (state_dim, state_dim)\n",
    "        lambda_steps: Step sizes ε_j, shape (n_lambda,)\n",
    "        lambda_values: Cumulative lambda values λ_j, shape (n_lambda,)\n",
    "        eta_bar_mu_0: Mean trajectory η̄_0, shape (state_dim, 1)\n",
    "        model_params: Dictionary with observation model\n",
    "\n",
    "    Returns:\n",
    "        particles_flowed: Updated particles, shape (state_dim, n_particle)\n",
    "    \"\"\"\n",
    "    # Initialize flow\n",
    "    eta = tf.identity(particles)  # Current particle positions\n",
    "    n_lambda = len(lambda_steps)\n",
    "    \n",
    "    # Initialize auxiliary trajectory for global linearization\n",
    "    eta_bar = tf.identity(eta_bar_mu_0)\n",
    "    \n",
    "    # Algorithm Line 10: Set λ = 0\n",
    "    # Algorithm Line 11: for j = 1, ..., N_λ do\n",
    "    for j in range(n_lambda):\n",
    "        # Algorithm Line 12: Set λ = λ + ε_j\n",
    "        epsilon_j = lambda_steps[j].numpy()   # Step size\n",
    "        lambda_j = lambda_values[j]   # Current lambda value\n",
    "        \n",
    "        # Algorithm Line 13: Compute A, b ONCE at global mean η̄\n",
    "        A, b = compute_flow_parameters(eta_bar, eta_bar_mu_0, P_pred, measurement,\n",
    "                                      lambda_j, model_params)\n",
    "        \n",
    "        # Algorithm Line 14: Migrate η̄\n",
    "        slope_bar = tf.linalg.matvec(A, tf.squeeze(eta_bar)) + b\n",
    "        eta_bar = eta_bar + epsilon_j * tf.expand_dims(slope_bar, 1)\n",
    "        \n",
    "        # Algorithm Lines 15-17: Migrate all particles using the same A, b\n",
    "        slopes = tf.matmul(A, eta) + tf.expand_dims(b, 1)\n",
    "        eta = eta + epsilon_j * slopes\n",
    "    \n",
    "    return eta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function 8: Compute Likelihood\n",
    "\n",
    "**Algorithm Line 24 (LEDH) / Line 21 (EDH)**\n",
    "\n",
    "**Purpose**: Compute likelihood $p(z_k|x_k)$ for weight update.\n",
    "\n",
    "**Equation** (for Gaussian measurement noise):\n",
    "$$p(z_k|x_k) = \\mathcal{N}(z_k; h(x_k), R)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def compute_likelihood(x, measurement, model_params):\n    \"\"\"\n    Compute likelihood p(z_k|x_k) for Gaussian measurement model.\n    \n    Equation:\n      p(z_k|x_k) = N(z_k; h(x_k), R)\n    \n    Args:\n        x: State, shape (state_dim, 1)\n        measurement: Measurement z_k, shape (n_sensor, 1)\n        model_params: Dictionary with observation model and R\n    \n    Returns:\n        likelihood: Scalar likelihood value\n    \"\"\"\n    R = model_params['R']\n    n_sensor = model_params['n_sensors']\n    \n    # Predicted measurement: h(x)\n    z_pred = observation_model(x, model_params, no_noise=True)\n    \n    # Innovation: z - h(x)\n    innovation = measurement - z_pred\n    \n    # Add regularization for numerical stability\n    regularization = 1e-6 * tf.eye(n_sensor, dtype=tf.float32)\n    R_regularized = R + regularization\n    \n    # Gaussian likelihood: exp(-0.5 * innovation^T * R^{-1} * innovation) / sqrt((2π)^n |R|)\n    # Use Cholesky for numerical stability\n    R_chol = tf.linalg.cholesky(R_regularized)\n    alpha = tf.linalg.cholesky_solve(R_chol, innovation)\n    \n    # Mahalanobis distance\n    mahalanobis = tf.matmul(tf.transpose(innovation), alpha)\n    \n    # Log determinant using Cholesky\n    log_det_R = 2.0 * tf.reduce_sum(tf.math.log(tf.linalg.diag_part(R_chol)))\n    \n    # Log likelihood\n    log_likelihood = -0.5 * (mahalanobis[0, 0] + log_det_R + \n                             tf.cast(n_sensor, tf.float32) * tf.math.log(2.0 * np.pi))\n    \n    # Clamp log likelihood to prevent extreme values\n    log_likelihood = tf.clip_by_value(log_likelihood, -100.0, 100.0)\n    \n    return tf.exp(log_likelihood)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function 9: PFPF Filter Step (LEDH)\n",
    "\n",
    "**Algorithm Lines 3-29 (LEDH)**\n",
    "\n",
    "**Purpose**: Perform one complete PFPF-LEDH filter step with importance weight update.\n",
    "\n",
    "**Key Steps**:\n",
    "1. Propagate particles (prediction)\n",
    "2. Estimate covariance for each particle\n",
    "3. Particle flow using local linearization\n",
    "4. **Weight update with invertible mapping**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def pfpf_ledh_filter_step(particles, weights, measurement, lambda_steps, lambda_values, \n                          model_params):\n    \"\"\"\n    Perform one PFPF-LEDH filter step with local linearization.\n    \n    Algorithm Lines 4-29 (LEDH):\n      4-10:  Propagate particles and initialize flow\n      11-21: Particle flow with local linearization\n      22-26: Weight update using invertible mapping\n      27-28: EKF update and state estimation\n    \n    Args:\n        particles: Current particles, shape (state_dim, n_particle)\n        weights: Current weights, shape (n_particle,)\n        measurement: Current measurement, shape (n_sensor, 1)\n        lambda_steps: Step sizes, shape (n_lambda,)\n        lambda_values: Lambda values, shape (n_lambda,)\n        model_params: Dictionary with model parameters\n    \n    Returns:\n        particles_updated: Updated particles, shape (state_dim, n_particle)\n        weights_updated: Updated weights, shape (n_particle,)\n        mean_estimate: State estimate, shape (state_dim,)\n    \"\"\"\n    n_particle = tf.shape(particles)[1].numpy()\n    state_dim = model_params['state_dim']\n    \n    # Storage for per-particle quantities\n    P_pred_list = []\n    eta_bar_mu_0_list = []\n    particles_pred_list = []\n    \n    # Algorithm Lines 4-10: Propagate each particle and estimate its covariance\n    for i in range(n_particle):\n        x_i = tf.expand_dims(particles[:, i], 1)  # (state_dim, 1)\n        \n        # Algorithm Line 6: Calculate η̄^i = g_k(x_{k-1}^i, 0)\n        eta_bar_mu_0_i = state_transition(x_i, model_params, no_noise=True)\n        \n        # Algorithm Line 7: Propagate particle: η_0^i = g_k(x_{k-1}^i, v_k)\n        eta_0_i = state_transition(x_i, model_params, use_real_noise=False)\n        \n        # Algorithm Line 5: Estimate P^i from particles (simplified: use same P for all)\n        # In practice, could maintain per-particle covariance\n        # For now, we'll estimate from particle spread\n        P_i = estimate_covariance(particles)\n        \n        P_pred_list.append(P_i)\n        eta_bar_mu_0_list.append(eta_bar_mu_0_i)\n        particles_pred_list.append(tf.squeeze(eta_0_i, axis=1))\n    \n    particles_pred = tf.stack(particles_pred_list, axis=1)\n    \n    # Algorithm Lines 11-21: Particle flow with local linearization\n    particles_flowed, theta_values = particle_flow_ledh(\n        particles_pred, measurement, P_pred_list, lambda_steps, lambda_values,\n        eta_bar_mu_0_list, model_params\n    )\n    \n    # Algorithm Lines 22-26: Weight update using LOG-SPACE for numerical stability\n    Q = model_params['Q']\n    Q_chol = tf.linalg.cholesky(Q)\n    log_det_Q = 2.0 * tf.reduce_sum(tf.math.log(tf.linalg.diag_part(Q_chol)))\n    \n    log_weights_new = []\n    for i in range(n_particle):\n        x_k_i = tf.expand_dims(particles_flowed[:, i], 1)  # (state_dim, 1)\n        eta_0_i = tf.expand_dims(particles_pred[:, i], 1)  # (state_dim, 1)\n        x_km1_i = tf.expand_dims(particles[:, i], 1)  # (state_dim, 1)\n        \n        # Predicted mean for this particle\n        x_pred_mean = state_transition(x_km1_i, model_params, no_noise=True)\n        \n        # LOG p(x_k^i|x_{k-1}^i)\n        diff_pred = x_k_i - x_pred_mean\n        alpha_pred = tf.linalg.cholesky_solve(Q_chol, diff_pred)\n        log_p_transition = -0.5 * (tf.matmul(tf.transpose(diff_pred), alpha_pred)[0, 0] + \n                                   log_det_Q + state_dim * tf.math.log(2.0 * np.pi))\n        \n        # LOG p(z_k|x_k^i) - compute directly in log space\n        R = model_params['R']\n        n_sensor = model_params['n_sensors']\n        z_pred = observation_model(x_k_i, model_params, no_noise=True)\n        innovation = measurement - z_pred\n        regularization = 1e-6 * tf.eye(n_sensor, dtype=tf.float32)\n        R_regularized = R + regularization\n        R_chol = tf.linalg.cholesky(R_regularized)\n        alpha_meas = tf.linalg.cholesky_solve(R_chol, innovation)\n        mahalanobis = tf.matmul(tf.transpose(innovation), alpha_meas)[0, 0]\n        log_det_R = 2.0 * tf.reduce_sum(tf.math.log(tf.linalg.diag_part(R_chol)))\n        log_p_likelihood = -0.5 * (mahalanobis + log_det_R + \n                                   tf.cast(n_sensor, tf.float32) * tf.math.log(2.0 * np.pi))\n        \n        # LOG p(η_0^i|x_{k-1}^i)\n        diff_eta = eta_0_i - x_pred_mean\n        alpha_eta = tf.linalg.cholesky_solve(Q_chol, diff_eta)\n        log_p_eta = -0.5 * (tf.matmul(tf.transpose(diff_eta), alpha_eta)[0, 0] + \n                           log_det_Q + state_dim * tf.math.log(2.0 * np.pi))\n        \n        # LOG θ^i (determinant term)\n        log_theta = tf.math.log(theta_values[i] + 1e-30)\n        \n        # Algorithm Line 24: Weight update in LOG space\n        # log(w_k^i) = log(p(x_k^i|x_{k-1}^i)) + log(p(z_k|x_k^i)) + log(θ^i) - log(p(η_0^i|x_{k-1}^i)) + log(w_{k-1}^i)\n        log_w_new = log_p_transition + log_p_likelihood + log_theta - log_p_eta + tf.math.log(weights[i] + 1e-30)\n        \n        # Clamp to prevent extreme values\n        log_w_new = tf.clip_by_value(log_w_new, -100.0, 100.0)\n        log_weights_new.append(log_w_new)\n    \n    log_weights_updated = tf.stack(log_weights_new)\n    \n    # Convert back from log space using log-sum-exp trick\n    max_log_weight = tf.reduce_max(log_weights_updated)\n    weights_updated = tf.exp(log_weights_updated - max_log_weight)\n    \n    # Algorithm Line 26: Normalize weights\n    weight_sum = tf.reduce_sum(weights_updated)\n    if weight_sum > 1e-30:\n        weights_updated = weights_updated / weight_sum\n    else:\n        # If all weights collapsed, reset to uniform\n        weights_updated = tf.ones(n_particle, dtype=tf.float32) / tf.cast(n_particle, tf.float32)\n    \n    # Algorithm Line 28: Estimate state\n    mean_estimate = tf.reduce_sum(particles_flowed * tf.expand_dims(weights_updated, 0), axis=1)\n    \n    return particles_flowed, weights_updated, mean_estimate"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function 10: PFPF Filter Step (EDH)\n",
    "\n",
    "**Algorithm Lines 3-27 (EDH)**\n",
    "\n",
    "**Purpose**: Perform one complete PFPF-EDH filter step with importance weight update.\n",
    "\n",
    "**Key Difference from LEDH**: Uses global linearization, no determinant computation needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def pfpf_edh_filter_step(particles, weights, measurement, lambda_steps, lambda_values, \n                         model_params, x_est_prev):\n    \"\"\"\n    Perform one PFPF-EDH filter step with global linearization.\n    \n    Algorithm Lines 3-27 (EDH):\n      4-9:   Propagate particles and compute global mean\n      10-18: Particle flow with global linearization\n      19-23: Weight update (simpler than LEDH, no determinant)\n      24-26: EKF update and state estimation\n    \n    Args:\n        particles: Current particles, shape (state_dim, n_particle)\n        weights: Current weights, shape (n_particle,)\n        measurement: Current measurement, shape (n_sensor, 1)\n        lambda_steps: Step sizes, shape (n_lambda,)\n        lambda_values: Lambda values, shape (n_lambda,)\n        model_params: Dictionary with model parameters\n        x_est_prev: Previous state estimate, shape (state_dim, 1)\n    \n    Returns:\n        particles_updated: Updated particles, shape (state_dim, n_particle)\n        weights_updated: Updated weights, shape (n_particle,)\n        mean_estimate: State estimate, shape (state_dim,)\n    \"\"\"\n    n_particle = tf.shape(particles)[1].numpy()\n    state_dim = model_params['state_dim']\n    \n    # Algorithm Line 4: Global covariance prediction\n    # (Could use EKF prediction here; for simplicity, estimate from particles)\n    P_pred = estimate_covariance(particles)\n    \n    # Algorithm Lines 5-8: Propagate particles\n    particles_pred = propagate_particles(particles, model_params)\n    \n    # Algorithm Line 9: Calculate global mean trajectory\n    eta_bar_mu_0 = state_transition(x_est_prev, model_params, no_noise=True)\n    \n    # Algorithm Lines 10-18: Particle flow with global linearization\n    particles_flowed = particle_flow_edh(\n        particles_pred, measurement, P_pred, lambda_steps, lambda_values,\n        eta_bar_mu_0, model_params\n    )\n    \n    # Algorithm Lines 19-23: Weight update using LOG-SPACE for numerical stability\n    Q = model_params['Q']\n    Q_chol = tf.linalg.cholesky(Q)\n    log_det_Q = 2.0 * tf.reduce_sum(tf.math.log(tf.linalg.diag_part(Q_chol)))\n    \n    log_weights_new = []\n    for i in range(n_particle):\n        x_k_i = tf.expand_dims(particles_flowed[:, i], 1)  # (state_dim, 1)\n        eta_0_i = tf.expand_dims(particles_pred[:, i], 1)  # (state_dim, 1)\n        x_km1_i = tf.expand_dims(particles[:, i], 1)  # (state_dim, 1)\n        \n        # Predicted mean for this particle\n        x_pred_mean = state_transition(x_km1_i, model_params, no_noise=True)\n        \n        # LOG p(x_k^i|x_{k-1}^i)\n        diff_pred = x_k_i - x_pred_mean\n        alpha_pred = tf.linalg.cholesky_solve(Q_chol, diff_pred)\n        log_p_transition = -0.5 * (tf.matmul(tf.transpose(diff_pred), alpha_pred)[0, 0] + \n                                   log_det_Q + state_dim * tf.math.log(2.0 * np.pi))\n        \n        # LOG p(z_k|x_k^i) - compute directly in log space\n        R = model_params['R']\n        n_sensor = model_params['n_sensors']\n        z_pred = observation_model(x_k_i, model_params, no_noise=True)\n        innovation = measurement - z_pred\n        regularization = 1e-6 * tf.eye(n_sensor, dtype=tf.float32)\n        R_regularized = R + regularization\n        R_chol = tf.linalg.cholesky(R_regularized)\n        alpha_meas = tf.linalg.cholesky_solve(R_chol, innovation)\n        mahalanobis = tf.matmul(tf.transpose(innovation), alpha_meas)[0, 0]\n        log_det_R = 2.0 * tf.reduce_sum(tf.math.log(tf.linalg.diag_part(R_chol)))\n        log_p_likelihood = -0.5 * (mahalanobis + log_det_R + \n                                   tf.cast(n_sensor, tf.float32) * tf.math.log(2.0 * np.pi))\n        \n        # LOG p(η_0^i|x_{k-1}^i)\n        diff_eta = eta_0_i - x_pred_mean\n        alpha_eta = tf.linalg.cholesky_solve(Q_chol, diff_eta)\n        log_p_eta = -0.5 * (tf.matmul(tf.transpose(diff_eta), alpha_eta)[0, 0] + \n                           log_det_Q + state_dim * tf.math.log(2.0 * np.pi))\n        \n        # Algorithm Line 21: Weight update in LOG space\n        # log(w_k^i) = log(p(x_k^i|x_{k-1}^i)) + log(p(z_k|x_k^i)) - log(p(η_0^i|x_{k-1}^i)) + log(w_{k-1}^i)\n        log_w_new = log_p_transition + log_p_likelihood - log_p_eta + tf.math.log(weights[i] + 1e-30)\n        \n        # Clamp to prevent extreme values\n        log_w_new = tf.clip_by_value(log_w_new, -100.0, 100.0)\n        log_weights_new.append(log_w_new)\n    \n    log_weights_updated = tf.stack(log_weights_new)\n    \n    # Convert back from log space using log-sum-exp trick\n    max_log_weight = tf.reduce_max(log_weights_updated)\n    weights_updated = tf.exp(log_weights_updated - max_log_weight)\n    \n    # Algorithm Line 23: Normalize weights\n    weight_sum = tf.reduce_sum(weights_updated)\n    if weight_sum > 1e-30:\n        weights_updated = weights_updated / weight_sum\n    else:\n        # If all weights collapsed, reset to uniform\n        weights_updated = tf.ones(n_particle, dtype=tf.float32) / tf.cast(n_particle, tf.float32)\n    \n    # Algorithm Line 25: Estimate state\n    mean_estimate = tf.reduce_sum(particles_flowed * tf.expand_dims(weights_updated, 0), axis=1)\n    \n    return particles_flowed, weights_updated, mean_estimate"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function 11: Run PFPF Filter\n",
    "\n",
    "**Algorithm Main Loop (Lines 3-30 LEDH / 3-27 EDH)**\n",
    "\n",
    "**Purpose**: Run PFPF filter on full measurement sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def run_pfpf_filter(measurements, model_params, n_particle=100, n_lambda=20, \n                    lambda_ratio=1.2, use_ledh=True, resample_threshold=0.5):\n    \"\"\"\n    Run PFPF filter on measurement sequence.\n    \n    Algorithm Main Loop:\n      1-2  Initialize particles and weights\n      3    for k = 1 to T do\n             [Lines 4-29 (LEDH) or 4-27 (EDH): filter step]\n           endfor\n    \n    Args:\n        measurements: Measurements, shape (n_sensor, T)\n        model_params: Dictionary with model parameters\n        n_particle: Number of particles\n        n_lambda: Number of lambda steps\n        lambda_ratio: Exponential spacing ratio\n        use_ledh: If True, use LEDH (local). If False, use EDH (global)\n        resample_threshold: Resample when ESS < resample_threshold * n_particle\n    \n    Returns:\n        estimates: State estimates, shape (state_dim, T)\n        particles_all: All particles, shape (state_dim, n_particle, T)\n        weights_all: All weights, shape (n_particle, T)\n    \"\"\"\n    T = tf.shape(measurements)[1].numpy()\n    state_dim = model_params['state_dim']\n    \n    filter_name = \"PFPF-LEDH\" if use_ledh else \"PFPF-EDH\"\n    print(f\"\\nRunning {filter_name} Filter:\")\n    print(f\"  Particles: {n_particle}\")\n    print(f\"  Lambda steps: {n_lambda}\")\n    print(f\"  Lambda ratio: {lambda_ratio}\")\n    print(f\"  Time steps: {T}\")\n    \n    # Pre-compute lambda steps\n    lambda_steps, lambda_values = compute_lambda_steps(n_lambda, lambda_ratio)\n    \n    # Algorithm Lines 1-2: Initialize particles and weights\n    particles, weights, m0, P = initialize_particles(model_params, n_particle)\n    x_est = m0  # Initial state estimate\n    \n    # Storage\n    estimates_list = []\n    particles_list = []\n    weights_list = []\n    \n    # Algorithm Line 3: for k = 1 to T do\n    print(\"\\nProcessing time steps...\")\n    for t in range(T):\n        if (t + 1) % 10 == 0:\n            print(f\"  Step {t+1}/{T}\")\n        \n        z_t = tf.expand_dims(measurements[:, t], 1)  # (n_sensor, 1)\n        \n        # Filter step\n        if use_ledh:\n            particles, weights, mean_estimate = pfpf_ledh_filter_step(\n                particles, weights, z_t, lambda_steps, lambda_values, model_params\n            )\n        else:\n            particles, weights, mean_estimate = pfpf_edh_filter_step(\n                particles, weights, z_t, lambda_steps, lambda_values, model_params, x_est\n            )\n        \n        x_est = tf.expand_dims(mean_estimate, 1)\n        \n        # Optional resampling\n        ess = 1.0 / tf.reduce_sum(tf.square(weights))\n        if ess < resample_threshold * n_particle:\n            # Systematic resampling - w is already a numpy array inside tf.numpy_function\n            indices = tf.numpy_function(\n                lambda w: np.random.choice(n_particle, n_particle, p=w),\n                [weights],\n                tf.int32\n            )\n            particles = tf.gather(particles, indices, axis=1)\n            weights = tf.ones(n_particle, dtype=tf.float32) / n_particle\n        \n        # Store results\n        estimates_list.append(mean_estimate)\n        particles_list.append(particles)\n        weights_list.append(weights)\n    \n    # Concatenate results\n    estimates = tf.stack(estimates_list, axis=1)\n    particles_all = tf.stack(particles_list, axis=2)\n    weights_all = tf.stack(weights_list, axis=1)\n    \n    print(f\"\\n{filter_name} filter completed successfully!\")\n    print(f\"  Estimates shape: {estimates.shape}\")\n    print(f\"  Particles shape: {particles_all.shape}\")\n    print(f\"  Weights shape: {weights_all.shape}\")\n    \n    return estimates, particles_all, weights_all"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate Ground Truth Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate ground truth trajectory and measurements\n",
    "T = 10  # Number of time steps\n",
    "ground_truth, measurements = simulate_trajectory(model_params, T, keep_in_bounds=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run PFPF Filters\n",
    "\n",
    "Now we run both versions of the filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run PFPF-EDH filter (global linearization - faster)\n",
    "pfpf_edh_estimates, pfpf_edh_particles, pfpf_edh_weights = run_pfpf_filter(\n",
    "    measurements=measurements,\n",
    "    model_params=model_params,\n",
    "    n_particle=100,\n",
    "    n_lambda=20,\n",
    "    lambda_ratio=1.2,\n",
    "    use_ledh=False  # EDH: Global linearization\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compute Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_position_error(estimates, ground_truth, n_targets):\n",
    "    \"\"\"Compute position error for each target.\"\"\"\n",
    "    errors = []\n",
    "    \n",
    "    for i in range(n_targets):\n",
    "        x_idx = i * 4\n",
    "        y_idx = i * 4 + 1\n",
    "        \n",
    "        x_err = estimates[x_idx, :] - ground_truth[x_idx, :]\n",
    "        y_err = estimates[y_idx, :] - ground_truth[y_idx, :]\n",
    "        \n",
    "        error = tf.sqrt(x_err**2 + y_err**2)\n",
    "        errors.append(error)\n",
    "    \n",
    "    errors = tf.stack(errors, axis=0)  # (n_targets, T)\n",
    "    mean_error = tf.reduce_mean(errors, axis=0)  # (T,)\n",
    "    \n",
    "    return errors, mean_error\n",
    "\n",
    "# Compute errors\n",
    "pfpf_edh_errors, pfpf_edh_mean_error = compute_position_error(\n",
    "    pfpf_edh_estimates, ground_truth[:,1:], model_params['n_targets']\n",
    ")\n",
    "\n",
    "# Overall RMSE\n",
    "rmse = tf.sqrt(tf.reduce_mean((pfpf_edh_estimates - ground_truth[:,1:])**2))\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PFPF-EDH Filter Performance\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Overall RMSE: {rmse.numpy():.4f} m\")\n",
    "print(f\"Mean position error (final): {pfpf_edh_mean_error[-1].numpy():.4f} m\")\n",
    "print(f\"Mean position error (average): {tf.reduce_mean(pfpf_edh_mean_error).numpy():.4f} m\")\n",
    "print(\"\\nPer-target average errors:\")\n",
    "for i in range(model_params['n_targets']):\n",
    "    avg_err = tf.reduce_mean(pfpf_edh_errors[i]).numpy()\n",
    "    print(f\"  Target {i+1}: {avg_err:.4f} m\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot estimated trajectories\n",
    "fig, ax = plt.subplots(figsize=(11, 10))\n",
    "\n",
    "# Plot sensors\n",
    "sensors = model_params['sensor_positions'].numpy()\n",
    "ax.scatter(sensors[:, 0], sensors[:, 1], \n",
    "          c='blue', marker='o', s=100, label='Sensors', alpha=0.6, zorder=5)\n",
    "\n",
    "colors = ['red', 'green', 'purple', 'orange']\n",
    "gt = ground_truth.numpy()\n",
    "est = pfpf_edh_estimates.numpy()\n",
    "\n",
    "for i in range(model_params['n_targets']):\n",
    "    x_idx = i * 4\n",
    "    y_idx = i * 4 + 1\n",
    "    \n",
    "    # Ground truth\n",
    "    x_true = gt[x_idx, :]\n",
    "    y_true = gt[y_idx, :]\n",
    "    ax.plot(x_true, y_true, '-', color=colors[i], linewidth=2.5,\n",
    "           label=f'Target {i+1} (True)', alpha=0.7)\n",
    "    \n",
    "    # Estimates\n",
    "    x_est = est[x_idx, :]\n",
    "    y_est = est[y_idx, :]\n",
    "    ax.plot(x_est, y_est, '--', color=colors[i], linewidth=2.5,\n",
    "           label=f'Target {i+1} (PFPF-EDH)', alpha=0.9, linestyle='dashed')\n",
    "    \n",
    "    # Mark start\n",
    "    ax.plot(x_true[0], y_true[0], 'o', color=colors[i], \n",
    "           markersize=12, markeredgecolor='black', markeredgewidth=2)\n",
    "\n",
    "ax.set_xlabel('X (m)', fontsize=14)\n",
    "ax.set_ylabel('Y (m)', fontsize=14)\n",
    "ax.set_title('PFPF Filter: True vs Estimated Trajectories', fontsize=16, fontweight='bold')\n",
    "ax.legend(fontsize=11, loc='upper right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xlim([0, model_params['sim_area_size']])\n",
    "ax.set_ylim([0, model_params['sim_area_size']])\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary: Function-to-Algorithm Mapping\n",
    "\n",
    "| Function | Algorithm Lines | Purpose |\n",
    "|----------|----------------|----------|\n",
    "| `compute_lambda_steps()` | Pre-processing | Exponential spacing for lambda |\n",
    "| `initialize_particles()` | 1-2 | Draw particles from prior and set weights |\n",
    "| `propagate_particles()` | 7 (LEDH) / 6 (EDH) | Motion model prediction |\n",
    "| `estimate_covariance()` | 5 (LEDH) / 4 (EDH) | Compute covariance from particles |\n",
    "| `compute_flow_parameters()` | 16 (LEDH) / 13 (EDH) | Compute A(λ) and b(λ) |\n",
    "| `particle_flow_ledh()` | 11-21 (LEDH) | Migrate particles with local linearization |\n",
    "| `particle_flow_edh()` | 10-18 (EDH) | Migrate particles with global linearization |\n",
    "| `compute_likelihood()` | 24 (LEDH) / 21 (EDH) | Compute p(z_k\\|x_k) for weights |\n",
    "| `pfpf_ledh_filter_step()` | 4-29 (LEDH) | One complete LEDH iteration |\n",
    "| `pfpf_edh_filter_step()` | 4-27 (EDH) | One complete EDH iteration |\n",
    "| `run_pfpf_filter()` | 1-30 (LEDH) / 1-27 (EDH) | Full filter on sequence |\n",
    "\n",
    "### Key Points:\n",
    "\n",
    "1. **Invertible Mapping Property**:\n",
    "   - LEDH: Weight update includes determinant term $\\theta^i = \\prod_j |\\det(I + \\epsilon_j A_j^i(\\lambda))|^{-1}$\n",
    "   - EDH: No determinant needed because same mapping for all particles\n",
    "\n",
    "2. **Weight Update**:\n",
    "   - LEDH: $w_k^i \\propto \\frac{p(x_k^i|x_{k-1}^i)p(z_k|x_k^i)\\theta^i}{p(\\eta_0^i|x_{k-1}^i)} w_{k-1}^i$\n",
    "   - EDH: $w_k^i \\propto \\frac{p(x_k^i|x_{k-1}^i)p(z_k|x_k^i)}{p(\\eta_0^i|x_{k-1}^i)} w_{k-1}^i$\n",
    "\n",
    "3. **Computational Complexity**:\n",
    "   - LEDH: $O(N_p S^3 N_\\lambda)$ - expensive due to per-particle flow\n",
    "   - EDH: $O(S^3 N_\\lambda + N_p)$ - much faster, shared flow parameters\n",
    "\n",
    "4. **Theoretical Guarantees**:\n",
    "   - PFPF maintains particle filter convergence guarantees\n",
    "   - Particle flow constructs good proposal distribution\n",
    "   - Weight update corrects for flow approximations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}