{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09c4f648",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16c000a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 1. Generate Synthetic Dataset\n",
    "# ==========================================\n",
    "\n",
    "N = 1000\n",
    "\n",
    "X0 = np.random.randn(N//2, 2) + np.array([-2, -2])\n",
    "X1 = np.random.randn(N//2, 2) + np.array([2, 2])\n",
    "\n",
    "X = np.vstack([X0, X1]).astype(np.float32)\n",
    "y = np.vstack([\n",
    "    np.zeros((N//2, 1)),\n",
    "    np.ones((N//2, 1))\n",
    "]).astype(np.float32)\n",
    "\n",
    "# Shuffle\n",
    "idx = np.random.permutation(N)\n",
    "X = X[idx]\n",
    "y = y[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc04edfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train / Test Split\n",
    "train_size = int(0.8 * N)\n",
    "\n",
    "X_train = tf.constant(X[:train_size])\n",
    "y_train = tf.constant(y[:train_size])\n",
    "\n",
    "X_test = tf.constant(X[train_size:])\n",
    "y_test = tf.constant(y[train_size:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36e6441",
   "metadata": {},
   "source": [
    "# 1. Using the with tf.GradientTape() as tape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5785ee36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 2. Define Model Class\n",
    "# ==========================================\n",
    "\n",
    "class SimpleMLP(tf.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Layer 1 parameters\n",
    "        self.W1 = tf.Variable(\n",
    "            tf.random.normal([input_dim, hidden_dim], stddev=0.1)\n",
    "        )\n",
    "        self.b1 = tf.Variable(tf.zeros([hidden_dim]))\n",
    "        \n",
    "        # Layer 2 parameters\n",
    "        self.W2 = tf.Variable(\n",
    "            tf.random.normal([hidden_dim, output_dim], stddev=0.1)\n",
    "        )\n",
    "        self.b2 = tf.Variable(tf.zeros([output_dim]))\n",
    "    \n",
    "    # Forward pass\n",
    "    def __call__(self, X):\n",
    "        h = tf.nn.relu(tf.matmul(X, self.W1) + self.b1)\n",
    "        logits = tf.matmul(h, self.W2) + self.b2\n",
    "        return logits\n",
    "    \n",
    "    # Loss function\n",
    "    def compute_loss(self, X, y):\n",
    "        logits = self(X)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "        )\n",
    "        return loss\n",
    "    \n",
    "    # One training step\n",
    "    def train_step(self, X, y, learning_rate):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self.compute_loss(X, y)\n",
    "        \n",
    "        grads = tape.gradient(loss, self.trainable_variables)\n",
    "        \n",
    "        # Backpropagation\n",
    "        for var, grad in zip(self.trainable_variables, grads):\n",
    "            var.assign_sub(learning_rate * grad)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    # Accuracy\n",
    "    def accuracy(self, X, y):\n",
    "        logits = self(X)\n",
    "        probs = tf.sigmoid(logits)\n",
    "        preds = tf.cast(probs > 0.5, tf.float32)\n",
    "        return tf.reduce_mean(\n",
    "            tf.cast(tf.equal(preds, y), tf.float32)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f16691c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 3. Train Model\n",
    "# ==========================================\n",
    "\n",
    "model = SimpleMLP(input_dim=2, hidden_dim=16, output_dim=1)\n",
    "\n",
    "epochs = 200\n",
    "lr = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed245296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.6427\n",
      "Epoch 20, Loss: 0.6124\n",
      "Epoch 40, Loss: 0.5812\n",
      "Epoch 60, Loss: 0.5490\n",
      "Epoch 80, Loss: 0.5158\n",
      "Epoch 100, Loss: 0.4821\n",
      "Epoch 120, Loss: 0.4481\n",
      "Epoch 140, Loss: 0.4143\n",
      "Epoch 160, Loss: 0.3808\n",
      "Epoch 180, Loss: 0.3480\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    loss = model.train_step(X_train, y_train, lr)\n",
    "    \n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.numpy():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14e1a735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Results:\n",
      "Train Accuracy: 0.99875\n",
      "Test Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 4. Evaluation\n",
    "# ==========================================\n",
    "\n",
    "train_acc = model.accuracy(X_train, y_train)\n",
    "test_acc = model.accuracy(X_test, y_test)\n",
    "\n",
    "print(\"\\nFinal Results:\")\n",
    "print(\"Train Accuracy:\", train_acc.numpy())\n",
    "print(\"Test Accuracy:\", test_acc.numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fd6bb9",
   "metadata": {},
   "source": [
    "# 2. With Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba076e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewSimpleMLP(tf.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.W1 = tf.Variable(tf.random.normal([input_dim, hidden_dim], stddev=0.1))\n",
    "        self.b1 = tf.Variable(tf.zeros([hidden_dim]))\n",
    "        \n",
    "        self.W2 = tf.Variable(tf.random.normal([hidden_dim, output_dim], stddev=0.1))\n",
    "        self.b2 = tf.Variable(tf.zeros([output_dim]))\n",
    "    \n",
    "    def __call__(self, X):\n",
    "        h = tf.nn.relu(tf.matmul(X, self.W1) + self.b1)\n",
    "        logits = tf.matmul(h, self.W2) + self.b2\n",
    "        return logits\n",
    "    \n",
    "    def compute_loss(self, X, y):\n",
    "        logits = self(X)\n",
    "        return tf.reduce_mean(\n",
    "            tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "        )\n",
    "    \n",
    "    def accuracy(self, X, y):\n",
    "        logits = self(X)\n",
    "        probs = tf.sigmoid(logits)\n",
    "        preds = tf.cast(probs > 0.5, tf.float32)\n",
    "        return tf.reduce_mean(tf.cast(tf.equal(preds, y), tf.float32))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dceec8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "# optimizer = tf.keras.optimizers.SGD(0.01)\n",
    "# optimizer = tf.keras.optimizers.RMSprop(0.001)\n",
    "# optimizer = tf.keras.optimizers.Adagrad(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "84ffd6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 2. Training Step (Modern TF2)\n",
    "# ==========================================\n",
    "\n",
    "@tf.function\n",
    "def train_step(model, optimizer, X, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = model.compute_loss(X, y)\n",
    "\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac47a2d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.7011\n",
      "Epoch 20, Loss: 0.6608\n",
      "Epoch 40, Loss: 0.6149\n",
      "Epoch 60, Loss: 0.5581\n",
      "Epoch 80, Loss: 0.4893\n",
      "Epoch 100, Loss: 0.4126\n",
      "Epoch 120, Loss: 0.3367\n",
      "Epoch 140, Loss: 0.2693\n",
      "Epoch 160, Loss: 0.2141\n",
      "Epoch 180, Loss: 0.1711\n",
      "Final Accuracy: 0.998\n"
     ]
    }
   ],
   "source": [
    "# Create model and optimizer\n",
    "model = NewSimpleMLP(2, 16, 1)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(200):\n",
    "    loss = train_step(model, optimizer, X, y)\n",
    "    \n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.numpy():.4f}\")\n",
    "\n",
    "print(\"Final Accuracy:\", model.accuracy(X, y).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d8e27d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dpf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
