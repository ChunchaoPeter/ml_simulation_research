{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Particle Filter Implementation with TensorFlow and TensorFlow Probability\n",
    "\n",
    "## Model Definition\n",
    "**Dynamic/State Equation:**\n",
    "```\n",
    "X(k+T) = AX(k) + GW(k)\n",
    "```\n",
    "\n",
    "**Observation Equation:**\n",
    "```\n",
    "Y(k) = [sqrt(X(k)[0]^2 + X(k)[1]^2);  # Range\n",
    "        atan2(X(k)[1], X(k)[0])]  + V(k)  # Bearing\n",
    "```\n",
    "\n",
    "**Parameters:**\n",
    "- State: X = [pos_x, pos_y, vel_x, vel_y]\n",
    "- Process noise: W ~ N(0, Q)\n",
    "- Observation noise: V ~ N(0, R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tfd = tfp.distributions\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"TensorFlow Probability version: {tfp.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load observation data - exactly as in Julia code\n",
    "# T = collect(5:5:200)\n",
    "T_values = tf.constant([5., 10., 15., 20., 25., 30., 35., 40., 45., 50., 55., 60., 65., 70., 75., 80., \n",
    "                        85., 90., 95., 100., 105., 110., 115., 120., 125., 130., 135., 140., 145., 150., \n",
    "                        155., 160., 165., 170., 175., 180., 185., 190., 195., 200.], dtype=tf.float64)\n",
    "\n",
    "Range = tf.constant([\n",
    "    27.942258, 27.963234, 27.204243, 26.540936, 26.135477, 24.921334, 26.008057, 23.451562,\n",
    "    23.475078, 22.859580, 22.441457, 20.646783, 21.282532, 21.026488, 21.922655, 23.111669,\n",
    "    24.607487, 26.674349, 27.889730, 29.995459, 31.511762, 31.999494, 32.519595, 33.830023,\n",
    "    33.801987, 34.345191, 34.878209, 35.858721, 37.624024, 39.121417, 40.449665, 40.245695,\n",
    "    40.242868, 41.246978, 41.759315, 41.612828, 42.370687, 43.895030, 45.125174, 48.508522\n",
    "], dtype=tf.float64)\n",
    "\n",
    "Theta = tf.constant([\n",
    "    0.817212, 0.844120, 0.870304, 0.939488, 0.977246, 1.009955, 1.095079, 1.123509,\n",
    "    1.195098, 1.317959, 1.404055, 1.569062, 1.679964, 1.751688, 1.908451, 2.032702,\n",
    "    2.144561, 2.159674, 2.172540, 2.231149, 2.217351, 2.157791, 2.140403, 2.165825,\n",
    "    2.121799, 2.082291, 2.126691, 2.102974, 2.034597, 2.031375, 2.024936, 2.053166,\n",
    "    2.010958, 1.983078, 2.000593, 2.042380, 2.023929, 2.044431, 2.006127, 1.991178\n",
    "], dtype=tf.float64)\n",
    "\n",
    "# Use the first timestep value as T (in Julia: df.T[1])\n",
    "T_timestep = T_values[0]\n",
    "\n",
    "# Y_observation = hcat(df.Range, df.Theta) in Julia\n",
    "Y_observations = tf.stack([Range, Theta], axis=1)\n",
    "num_steps = tf.shape(Y_observations)[0].numpy()\n",
    "\n",
    "print(f\"Loaded {num_steps} observations\")\n",
    "print(f\"Time step T: {T_timestep.numpy()}\")\n",
    "print(f\"Observations shape: {Y_observations.shape}\")\n",
    "print(f\"First observation: Range={Range[0].numpy():.6f}, Theta={Theta[0].numpy():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters - exactly as in Julia code\n",
    "T = T_timestep\n",
    "\n",
    "# State transition matrix: A = [1 0 T 0; 0 1 0 T; 0 0 1 0; 0 0 0 1]\n",
    "A = tf.constant([\n",
    "    [1., 0., T, 0.],\n",
    "    [0., 1., 0., T],\n",
    "    [0., 0., 1., 0.],\n",
    "    [0., 0., 0., 1.]\n",
    "], dtype=tf.float64)\n",
    "\n",
    "# Process noise matrix: G = [0 0; 0 0; T 0; 0 T]\n",
    "G = tf.constant([\n",
    "    [0., 0.],\n",
    "    [0., 0.],\n",
    "    [T, 0.],\n",
    "    [0., T]\n",
    "], dtype=tf.float64)\n",
    "\n",
    "# Process noise covariance: Q = [0.001 0; 0 0.001]\n",
    "Q = tf.constant([\n",
    "    [0.001, 0.],\n",
    "    [0., 0.001]\n",
    "], dtype=tf.float64)\n",
    "\n",
    "# Observation noise covariance: R = [0.25 0; 0 5*10^(-4)]\n",
    "R = tf.constant([\n",
    "    [0.25, 0.],\n",
    "    [0., 5e-4]\n",
    "], dtype=tf.float64)\n",
    "\n",
    "# Create distributions\n",
    "state_noise_dist = tfd.MultivariateNormalFullCovariance(\n",
    "    loc=tf.zeros(2, dtype=tf.float64),\n",
    "    covariance_matrix=Q\n",
    ")\n",
    "\n",
    "print(\"Model parameters initialized\")\n",
    "print(f\"A shape: {A.shape}, G shape: {G.shape}\")\n",
    "print(f\"Q shape: {Q.shape}, R shape: {R.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions - matching Julia implementation\n",
    "\n",
    "def state_transition(X, W):\n",
    "    \"\"\"State transition function: X(k+1) = A*X(k) + G*W(k)\n",
    "    Julia: fcn_X_pre(X, W) = A * X + G * W\n",
    "    \n",
    "    Args:\n",
    "        X: State tensor of shape (N, 4)\n",
    "        W: Process noise tensor of shape (N, 2)\n",
    "    \n",
    "    Returns:\n",
    "        New state tensor of shape (N, 4)\n",
    "    \"\"\"\n",
    "    # For each particle: X_new = A @ X + G @ W\n",
    "    # Vectorized: (N,4) @ (4,4)^T + (N,2) @ (4,2)^T = (N,4) + (N,4)\n",
    "    return tf.matmul(X, A, transpose_b=True) + tf.matmul(W, G, transpose_b=True)\n",
    "\n",
    "def observation_function(X):\n",
    "    \"\"\"Compute expected observation from state\n",
    "    Julia: [abs.(X[1] + X[2] * im); atan(X[2], X[1])]\n",
    "    Note: Julia uses 1-indexing, so X[1]=pos_x, X[2]=pos_y\n",
    "    \n",
    "    Args:\n",
    "        X: State tensor of shape (N, 4) where X[:,0]=pos_x, X[:,1]=pos_y\n",
    "    \n",
    "    Returns:\n",
    "        Observation tensor of shape (N, 2) - [range, bearing]\n",
    "    \"\"\"\n",
    "    # Range: |pos_x + i*pos_y| = sqrt(pos_x^2 + pos_y^2)\n",
    "    range_pred = tf.sqrt(X[:, 0]**2 + X[:, 1]**2)\n",
    "    # Bearing: atan2(pos_y, pos_x)\n",
    "    bearing_pred = tf.atan2(X[:, 1], X[:, 0])\n",
    "    return tf.stack([range_pred, bearing_pred], axis=1)\n",
    "\n",
    "def observation_residual(X, Y):\n",
    "    \"\"\"Compute observation residual V = Y - h(X)\n",
    "    Julia: fcn_V_between_pre_and_ob(X, Y) = Y - [abs.(X[1] + X[2] * im); atan(X[2], X[1])]\n",
    "    \n",
    "    Args:\n",
    "        X: State tensor of shape (N, 4)\n",
    "        Y: Observation tensor of shape (N, 2)\n",
    "    \n",
    "    Returns:\n",
    "        Residual tensor of shape (N, 2)\n",
    "    \"\"\"\n",
    "    Y_pred = observation_function(X)\n",
    "    return Y - Y_pred\n",
    "\n",
    "def likelihood(V):\n",
    "    \"\"\"Compute likelihood P(Y | X) using Gaussian likelihood\n",
    "    Julia: fcn_probabilityOF_Y_conditioningON_X(V) = exp(-0.5 * V' * inv(R) * V)\n",
    "    \n",
    "    Args:\n",
    "        V: Residual tensor of shape (N, 2)\n",
    "    \n",
    "    Returns:\n",
    "        Likelihood tensor of shape (N,)\n",
    "    \"\"\"\n",
    "    # For each particle i: exp(-0.5 * V[i]^T * R^{-1} * V[i])\n",
    "    R_inv = tf.linalg.inv(R)\n",
    "    # Compute V @ R_inv for all particles: (N, 2) @ (2, 2) = (N, 2)\n",
    "    temp = tf.matmul(V, R_inv)\n",
    "    # Compute V^T * (R_inv * V) element-wise and sum: (N, 2) * (N, 2) -> sum -> (N,)\n",
    "    exponent = -0.5 * tf.reduce_sum(temp * V, axis=1)\n",
    "    return tf.exp(exponent)\n",
    "\n",
    "print(\"Functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_resample(weights):\n",
    "    \"\"\"Random resampling algorithm\n",
    "    Julia: fcn_random_Resample(vect_weights)\n",
    "    \n",
    "    The Julia implementation:\n",
    "    1. Compute CDF of weights\n",
    "    2. Generate N uniform random numbers\n",
    "    3. For each random number, find the first CDF bin it falls into\n",
    "    \n",
    "    Args:\n",
    "        weights: particle weights of shape (N,)\n",
    "    \n",
    "    Returns:\n",
    "        indices: indices of particles to duplicate, shape (N,)\n",
    "    \"\"\"\n",
    "    N = tf.shape(weights)[0]\n",
    "    \n",
    "    # Compute CDF: cdf_of_weight = cumsum(vect_weights)\n",
    "    cdf = tf.cumsum(weights)\n",
    "    \n",
    "    # Generate random uniform samples: vect_U = rand(uni_dist, length_of_weight)\n",
    "    u = tf.random.uniform((N,), 0.0, 1.0, dtype=tf.float64)\n",
    "    \n",
    "    # For each u, find the first CDF value >= u\n",
    "    # This matches the Julia loop logic\n",
    "    indices = tf.searchsorted(cdf, u, side='right')\n",
    "    \n",
    "    return indices\n",
    "\n",
    "print(\"Resampling function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize state and particles - exactly as in Julia\n",
    "# X = [20; 20; -0.2; 0]\n",
    "X_init = tf.constant([20., 20., -0.2, 0.], dtype=tf.float64)\n",
    "\n",
    "# covMat_original_state = [1.0 0 0 0; 0 1.0 0 0; 0 0 0.01 0; 0 0 0 0.01]\n",
    "P_init = tf.constant([\n",
    "    [1.0, 0., 0., 0.],\n",
    "    [0., 1.0, 0., 0.],\n",
    "    [0., 0., 0.01, 0.],\n",
    "    [0., 0., 0., 0.01]\n",
    "], dtype=tf.float64)\n",
    "\n",
    "# Number of particles: N = 100\n",
    "N = 100\n",
    "\n",
    "# Sample initial particles: pos_particles = rand(state_dist, N)\n",
    "initial_dist = tfd.MultivariateNormalFullCovariance(\n",
    "    loc=X_init,\n",
    "    covariance_matrix=P_init\n",
    ")\n",
    "\n",
    "particles = initial_dist.sample(N)  # Shape: (N, 4)\n",
    "# weights_particles = ones(Float64, N) * (1.0 / N)\n",
    "weights = tf.ones(N, dtype=tf.float64) / tf.cast(N, tf.float64)\n",
    "\n",
    "# Storage for results\n",
    "# X_estimation[1, :] = X and X_prediction[1, :] = X\n",
    "X_estimation_list = [X_init]\n",
    "X_prediction_list = [X_init]\n",
    "\n",
    "print(f\"Initialized {N} particles\")\n",
    "print(f\"Initial state: {X_init.numpy()}\")\n",
    "print(f\"Initial particle mean: {tf.reduce_mean(particles, axis=0).numpy()}\")\n",
    "print(f\"Number of observations: {num_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Particle Filter main loop - matching Julia implementation exactly\n",
    "# for i = 1:size(Y_observation, 1)\n",
    "\n",
    "for i in range(num_steps):\n",
    "    # 1. Importance Sampling - Propagate particles\n",
    "    # state_noise_particles = rand(state_noise_dist, N)\n",
    "    state_noise = state_noise_dist.sample(N)  # Shape: (N, 2)\n",
    "    \n",
    "    # for j = 1:N\n",
    "    #     pos_particles[:, j] = fcn_X_pre(pos_particles[:, j], state_noise_particles[:, j])\n",
    "    # end\n",
    "    particles = state_transition(particles, state_noise)\n",
    "    \n",
    "    # 2. Calculate prediction\n",
    "    # X_pre = mean(pos_particles, dims=2)\n",
    "    X_pred = tf.reduce_mean(particles, axis=0)\n",
    "    X_prediction_list.append(X_pred)\n",
    "    \n",
    "    # 3. Update weights using observation\n",
    "    # for j = 1:N\n",
    "    #     v = fcn_V_between_pre_and_ob(pos_particles[:, j], Y_observation[i, :])\n",
    "    #     weights_particles[j] = weights_particles[j] * fcn_probabilityOF_Y_conditioningON_X(v)\n",
    "    # end\n",
    "    Y_current = tf.tile(Y_observations[i:i+1, :], [N, 1])  # Broadcast observation to (N, 2)\n",
    "    V = observation_residual(particles, Y_current)\n",
    "    likelihood_values = likelihood(V)\n",
    "    \n",
    "    weights = weights * likelihood_values\n",
    "    \n",
    "    # 4. Normalize weights\n",
    "    # sum_of_weights = sum(weights_particles)\n",
    "    # weights_particles[j] = weights_particles[j] / sum_of_weights\n",
    "    weight_sum = tf.reduce_sum(weights)\n",
    "    if weight_sum < 1e-10:\n",
    "        print(f\"Warning: weights sum is nearly zero at iteration {i}\")\n",
    "        weights = tf.ones(N, dtype=tf.float64) / tf.cast(N, tf.float64)\n",
    "    else:\n",
    "        weights = weights / weight_sum\n",
    "    \n",
    "    # 5. Resampling\n",
    "    # duplicat_id = fcn_random_Resample(weights_particles)\n",
    "    # pos_particles = pos_particles[:, duplicat_id]\n",
    "    indices = random_resample(weights)\n",
    "    particles = tf.gather(particles, indices)\n",
    "    \n",
    "    # 6. Reset weights after resampling\n",
    "    # weights_particles = ones(Float64, N) * (1.0 / N)\n",
    "    weights = tf.ones(N, dtype=tf.float64) / tf.cast(N, tf.float64)\n",
    "    \n",
    "    # 7. Calculate estimation\n",
    "    # X_est = mean(pos_particles, dims=2)\n",
    "    X_est = tf.reduce_mean(particles, axis=0)\n",
    "    X_estimation_list.append(X_est)\n",
    "\n",
    "# Stack results into tensors\n",
    "X_estimation = tf.stack(X_estimation_list)  # Shape: (num_steps+1, 4)\n",
    "X_prediction = tf.stack(X_prediction_list)  # Shape: (num_steps+1, 4)\n",
    "\n",
    "print(\"\\nParticle filter completed\")\n",
    "print(f\"Estimation tensor shape: {X_estimation.shape}\")\n",
    "print(f\"Prediction tensor shape: {X_prediction.shape}\")\n",
    "print(f\"\\nFinal estimated state: {X_estimation[-1].numpy()}\")\n",
    "print(f\"Final predicted state: {X_prediction[-1].numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute observed positions for plotting\n",
    "# Julia: df.pos_x = @. df.Range * cos(df.Theta)\n",
    "#        df.pos_y = @. df.Range * sin(df.Theta)\n",
    "obs_pos_x = Range * tf.cos(Theta)\n",
    "obs_pos_y = Range * tf.sin(Theta)\n",
    "\n",
    "# Julia: pos_x = vcat(X[1], df.pos_x)\n",
    "#        pos_y = vcat(X[2], df.pos_y)\n",
    "obs_x_with_init = tf.concat([[X_init[0]], obs_pos_x], axis=0)\n",
    "obs_y_with_init = tf.concat([[X_init[1]], obs_pos_y], axis=0)\n",
    "\n",
    "# Create visualization matching Julia plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot observations (blue, dashed line with triangles)\n",
    "plt.plot(obs_x_with_init.numpy(), obs_y_with_init.numpy(), '--o', \n",
    "         color='blue', label='Observation', markersize=6, alpha=0.6)\n",
    "\n",
    "# Plot predictions (green, dashed line with triangles)\n",
    "plt.plot(X_prediction[:, 0].numpy(), X_prediction[:, 1].numpy(), '--^', \n",
    "         color='green', label='Prediction', markersize=6, alpha=0.6)\n",
    "\n",
    "# Plot estimations (red, solid line with squares)\n",
    "plt.plot(X_estimation[:, 0].numpy(), X_estimation[:, 1].numpy(), '-s', \n",
    "         color='red', label='Estimation', markersize=6, alpha=0.8)\n",
    "\n",
    "plt.xlabel('Position X (m)', fontsize=12)\n",
    "plt.ylabel('Position Y (m)', fontsize=12)\n",
    "plt.title('Particle Filter Results', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('pf_results.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"Visualization complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute estimation error statistics\n",
    "obs_positions = tf.stack([obs_pos_x, obs_pos_y], axis=1)  # Shape: (num_steps, 2)\n",
    "est_positions = X_estimation[1:, :2]  # Shape: (num_steps, 2), skip initial state\n",
    "pred_positions = X_prediction[1:, :2]  # Shape: (num_steps, 2)\n",
    "\n",
    "# Compute position errors\n",
    "estimation_errors = tf.norm(est_positions - obs_positions, axis=1)\n",
    "prediction_errors = tf.norm(pred_positions - obs_positions, axis=1)\n",
    "\n",
    "print(\"\\n=== Estimation Error Statistics ===\")\n",
    "print(f\"Mean error: {tf.reduce_mean(estimation_errors).numpy():.4f} m\")\n",
    "print(f\"Min error: {tf.reduce_min(estimation_errors).numpy():.4f} m\")\n",
    "print(f\"Max error: {tf.reduce_max(estimation_errors).numpy():.4f} m\")\n",
    "print(f\"Std error: {tfp.stats.stddev(estimation_errors).numpy():.4f} m\")\n",
    "\n",
    "print(\"\\n=== Prediction Error Statistics ===\")\n",
    "print(f\"Mean error: {tf.reduce_mean(prediction_errors).numpy():.4f} m\")\n",
    "print(f\"Min error: {tf.reduce_min(prediction_errors).numpy():.4f} m\")\n",
    "print(f\"Max error: {tf.reduce_max(prediction_errors).numpy():.4f} m\")\n",
    "print(f\"Std error: {tfp.stats.stddev(prediction_errors).numpy():.4f} m\")\n",
    "\n",
    "print(f\"\\nFinal estimated state (x, y, vx, vy): [{X_estimation[-1, 0].numpy():.3f}, {X_estimation[-1, 1].numpy():.3f}, {X_estimation[-1, 2].numpy():.3f}, {X_estimation[-1, 3].numpy():.3f}]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
